# -*- coding: utf-8 -*-
"""Deliv4+5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11mwf1sBSLzmZ4z6NIgspPGvIdY39NtXZ
"""

pip install vaderSentiment

pip install seaborn

pip install requests_html

pip install selenium

pip install kora -q

pip install chromedriver

pip install lxml

pip install html5lib

pip install webdriver_manager

#importing packages
import requests
from bs4 import BeautifulSoup as bs4
import pandas 
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import time
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from webdriver_manager.chrome import ChromeDriverManager
from selenium.webdriver.chrome.service import Service
import html5lib
import urllib.request
import json
from selenium.webdriver.common.desired_capabilities import DesiredCapabilities
from kora.selenium import wd

#put link inside quotations
page = requests.get("https://www.politico.com/news/magazine/2021/01/18/trump-presidency-administration-biggest-impact-policy-analysis-451479")
webpage = page.content
page.close()
soup = bs4(webpage, 'html.parser')
#print(soup)

info_box = soup.findAll("p",{"class":" story-text__paragraph"})
info_box=info_box[2:]
for i in range(len(info_box)):
  info_box[i]=info_box[i].text
info_box[13]=info_box[13] + '' + info_box[14]
info_box.pop(14)

from pandas.core.frame import DataFrame
list1=[["about","move","impact","upshoot"]]
for i in range(len(info_box)):
  if i%4==0:
    list1.append([info_box[i]])
  else:
    list1[-1].append(info_box[i])
df=pandas.DataFrame(list1)
df.columns = df.iloc[0]
df = df[1:]
df

list2=[]
list3=[]
# function to print sentiments
# of the sentence.
def sentiment_scores(sentence):
 
    # Create a SentimentIntensityAnalyzer object.
    sid_obj = SentimentIntensityAnalyzer()
 
    # polarity_scores method of SentimentIntensityAnalyzer
    # object gives a sentiment dictionary.
    # which contains pos, neg, neu, and compound scores.
    sentiment_dict = sid_obj.polarity_scores(sentence)
    if x == 1:
      return sentiment_dict['pos']*100
    else:
      return sentiment_dict['neg']*100
for i2 in range(len(df)):
  x=1
  list2.append(sentiment_scores(df["impact"].iloc[i2]))
for i2 in range(len(df)):
  x=2
  list3.append(sentiment_scores(df["impact"].iloc[i2]))

df["pos"]=list2
df["neg"]=list3
csvfile = df.to_csv('df.csv',index=False)

mean1=sum(list2)/len(list2)
mean2=0
mean3=0
for i2 in range(len(df)):
  mean2=mean2+sentiment_scores(df["upshoot"].iloc[i2])
  mean3=mean3+sentiment_scores(df["move"].iloc[i2])
mean2=mean2/len(df)
mean3=mean3/len(df)
average=mean1+mean2+mean3
average3=average/3
average3
#chose to do avarage since I felt it made the most sense

#put link inside quotations
page2 = requests.get("https://www.bbc.com/news/topics/cp7r8vgl2lgt/donald-trump")
webpage2 = page2.content
page.close()
soup2 = bs4(webpage2, 'html.parser')
#print(soup)

Politician_herf = []
Politicians = soup2.findAll("h3",{"class": "lx-stream-post__header-title gel-great-primer-bold qa-post-title gs-u-mt0 gs-u-mb-"})
#this statement gets just the herfs, I was having trouble doing it the other way so I figured out a differant way
for i3 in range(len(Politicians)):
   for herfs in Politicians[i3].findAll("a",href=True):
    Politician_herf.append('https://www.bbc.com' + herfs.get('href'))
Politician_herf

title = []
articaltext = []
bbcpos = []
for i2 in range(len(Politician_herf)):
  page3 = requests.get(Politician_herf[i2])
  webpage3 = page3.content
  page3.close()
  soup3 = bs4(webpage3, 'html.parser')
  text = soup3.findAll("div",{"class": "ssrcss-uf6wea-RichTextComponentWrapper e1xue1i85"})
  z=soup3.find("h1",{"class": "ssrcss-gcq6xq-StyledHeading e1fj1fc10"})
  if (len(str(z))>4):
    articaltext.append([])
    title.append([soup3.find("h1",{"class": "ssrcss-gcq6xq-StyledHeading e1fj1fc10"}).text])
    for i3 in range(len(text)):
      articaltext[-1].append(text[i3].text)
    bbcpos.append([sentiment_scores(articaltext[-1])])
df2=pandas.DataFrame({"title":title,"articaltext":articaltext,"bbcpos":bbcpos})
df2

sum=0
for i2 in bbcpos:
  for i3 in i2:
    #I did this to access the contents of the nested list
    sum=sum+i3
average2=sum/len(bbcpos)
average2

ratings= [["politico","bbc","CNN","Fox News","huffpost","The Washington Post","NPR"],[average3,average2]]
links = ["https://www.cnn.com/search?size=10&q=Donald%20Trump&category=politics&sort=relevance&type=article","https://www.foxnews.com/search-results/search?q=Donald%20Trump","https://www.huffpost.com/news/topic/donald-trump","https://www.washingtonpost.com/search?query=donald%20trump&btn-search=&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D"]
for i in range(len(links)):
  print("0-" + str(i))
  average_list=[]
  x=1
  mean=0
  artical=""

  options = webdriver.ChromeOptions()
  options.add_argument('--headless')
  options.add_argument('--no-sandbox')
  options.add_argument('--disable-dev-shm-usage')
  # open it, go to a website, and get results
  wd = webdriver.Chrome(options=options)
  wd.get(links[i])
  webpage=wd.page_source
  wd.quit()
  soup = bs4(webpage)  
  Politician_herf = []

  if (i==0):
    Politicians = soup.findAll("h3",{"class": "cnn-search__result-headline"})
    for i2 in range(len(Politicians)):
      herf=Politicians[i2].a.get('href')
      Politician_herf.append('https:' + str(herf))
    for i3 in range(len(Politician_herf)):
      artical="" 
      page = requests.get(Politician_herf[i3])
      time.sleep(3) # Sleep for 3 seconds
      webpage = page.content
      page.close()
      soup = bs4(webpage, "html.parser")
      text=soup.findAll("p",{"class": "paragraph inline-placeholder"})
      if (len(text)==0):
        text=soup.find("div",{"class": "l-container"})
        if (str(type(text))!="<class 'NoneType'>"):
          for i4 in text:
            artical = artical + "" + i4.text
      else:
        for i4 in range(len(text)):
          artical = artical + "" + text[i4].text
      mean = mean + sentiment_scores(artical)
    if (len(Politician_herf)==0):
      ratings[-1].append("sourse code is JavaScript")
    else:
      average=mean/len(Politician_herf)
      ratings[-1].append(average)

  if (i==1):
    Politicians = soup.findAll("h2",{"class": "title"})
    for i2 in range(len(Politicians)):
      for herfs in Politicians[i2].findAll("a",href=True):
        Politician_herf.append(herfs.get('href'))
    for i3 in range(len(Politician_herf)):  
      artical=""
      page = requests.get(Politician_herf[i3])
      time.sleep(3) # Sleep for 3 seconds
      webpage = page.content
      page.close()
      soup = bs4(webpage, "html.parser")
      text=soup.findAll("p")
      for i4 in range(len(text)):
        artical= artical + "" + text[i4].text
      mean = mean + sentiment_scores(artical)
    if (len(Politician_herf)==0):
      ratings[-1].append("sourse code is JavaScript")
    else:
      average=mean/len(Politician_herf)
      ratings[-1].append(average)
      
  if (i==2):
    Politicians = soup.body.findAll("a",{"class":"card__headline card__headline--long"})
    for i2 in Politicians:
        Politician_herf.append(i2['href'])
    for i3 in Politician_herf: 
      artical=""
      options = webdriver.ChromeOptions()
      options.add_argument('--headless')
      options.add_argument('--no-sandbox')
      options.add_argument('--disable-dev-shm-usage')
      # open it, go to a website, and get results
      wd = webdriver.Chrome(options=options)
      wd.get(i3)
      webpage=wd.page_source
      wd.quit()
      soup = bs4(webpage)
      text=soup.findAll("div",{"class": "primary-cli cli cli-text "})
      for i4 in range(len(text)):
        artical= artical + "" + text[i4].text
      mean = mean + sentiment_scores(artical)
    if (len(Politician_herf)==0):
      ratings[-1].append("sourse code is JavaScript")
    else:
      average=mean/len(Politician_herf)
      ratings[-1].append(average)
            
  if (i==3):
    Politicians = soup.findAll("div",{"class": "pr-sm flex flex-column justify-between w-100"})
    for i2 in range(len(Politicians)):
      for herfs in Politicians[i2].findAll("a",href=True):
        Politician_herf.append(herfs.get('href'))
    for i3 in range(len(Politician_herf)):  
      artical=""
      page = requests.get(Politician_herf[i3])
      time.sleep(3) # Sleep for 3 seconds
      webpage = page.content
      page.close()
      soup = bs4(webpage, "html.parser")
      text=soup.findAll("p",{"class": "font-copy font--article-body gray-darkest ma-0 pb-md"})
      for i4 in range(len(text)):
        artical= artical + "" + text[i4].text
      mean = mean + sentiment_scores(artical)
    if (len(Politician_herf)==0):
      ratings[-1].append("sourse code is JavaScript")
    else:
      average=mean/len(Politician_herf)
      ratings[-1].append(average)
          
  print(ratings[-1][-1])





links_Joe_Biden = ["https://www.politico.com/search?adv=true&userInitiated=true&s=&q=Joe+Biden&pv=&c=0000016c-7218-df3f-a1fe-779a301e0006&r=&start=&start_submit=&end=&end_submit=","https://www.bbc.co.uk/search?q=Joe+Biden","https://www.cnn.com/search?q=Joe%20Biden&size=10&category=politics&type=article","https://www.foxnews.com/search-results/search?q=Joe%20Biden","https://www.huffpost.com/news/topic/joe-biden","https://www.washingtonpost.com/search?query=Joe%20Biden&btn-search=&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%22Politics%22%5D%2C%22author%22%3A%5B%5D%7D","https://www.npr.org/search?query=Joe%20Biden&page=1"]
links_Barack_Obama = ["https://www.politico.com/search?adv=true&userInitiated=true&s=&q=Barack+Obama&pv=&c=0000016c-7218-df3f-a1fe-779a301e0006&r=&start=&start_submit=&end=&end_submit=","https://www.bbc.co.uk/search?q=Barack+Obama&page=1","https://www.cnn.com/search?size=10&q=Barack%20Obama&type=article&category=politics","https://www.foxnews.com/search-results/search?q=Barack%20Obama","https://www.huffpost.com/news/topic/barack-obama","https://www.washingtonpost.com/search?query=Barack%20Obama&btn-search=&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D","https://www.npr.org/search?query=Barack%20Obama&page=1"]
links_George_W_Bush = ["https://www.politico.com/search?adv=true&userInitiated=true&s=&q=George+W.+Bush&pv=&c=0000016c-7218-df3f-a1fe-779a301e0006&r=&start=&start_submit=&end=&end_submit=","https://www.bbc.co.uk/search?q=George+W.+Bush&page=1","https://www.cnn.com/search?size=10&q=George%20W.%20Bush&category=politics&type=article","https://www.foxnews.com/search-results/search?q=George%20W.%20Bush","https://www.huffpost.com/news/topic/george-w-bush","https://www.washingtonpost.com/search?query=George%20W.%20Bush&btn-search=&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D","https://www.npr.org/search?query=George%20W.%20Bush&page=1"]
links_William_J_Clinton = ["https://www.politico.com/search?adv=true&userInitiated=true&s=&q=William+J.+Clinton&pv=&c=0000016c-7218-df3f-a1fe-779a301e0006&r=&start=&start_submit=&end=&end_submit=","https://www.bbc.co.uk/search?q=William+J.+Clinton&page=1","https://www.cnn.com/search?size=10&q=William%20J.%20Clinton&type=article&category=politics","https://www.foxnews.com/search-results/search?q=William%20J.%20Clinton","https://www.huffpost.com/news/topic/bill-clinton","https://www.washingtonpost.com/search?query=William%20J.%20Clinton&btn-search=&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D","https://www.npr.org/search?query=William%20J.%20Clinton&page=1"]
links_Ronald_Reagan = ["https://www.politico.com/search?adv=true&userInitiated=true&s=&q=Ronald+Reagan&pv=&c=0000016c-7218-df3f-a1fe-779a301e0006&r=&start=&start_submit=&end=&end_submit=","https://www.bbc.co.uk/search?q=Ronald+Reagan&page=1","https://www.cnn.com/search?size=10&q=Ronald%20Reagan&category=politics&type=article","https://www.foxnews.com/search-results/search?q=Ronald%20Reagan","https://www.huffpost.com/news/topic/ronald-reagan","https://www.washingtonpost.com/search?query=Ronald%20Reagan&btn-search=&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D","https://www.npr.org/search?query=Ronald%20Reagan&page=1"]
links_James_Carter = ["https://www.politico.com/search?adv=true&userInitiated=true&s=&q=James+Carter&pv=0000014e-a307-d012-a3fe-bb87927c0000&c=0000016c-7218-df3f-a1fe-779a301e0006&r=&start=&start_submit=&end=&end_submit=","https://www.bbc.co.uk/search?q=James+Carter&page=1","https://www.cnn.com/search?size=10&q=James%20Carter&type=article&category=politics","https://www.foxnews.com/search-results/search?q=James%20Carter","https://www.huffpost.com/news/topic/jimmy-carter","https://www.washingtonpost.com/search?query=James%20Carter&btn-search=&facets=%7B%22time%22%3A%22all%22%2C%22sort%22%3A%22relevancy%22%2C%22section%22%3A%5B%5D%2C%22author%22%3A%5B%5D%7D","https://www.npr.org/search?query=James%20Carter&page=1"]

links = [links_Joe_Biden,links_Barack_Obama,links_George_W_Bush,links_William_J_Clinton,links_Ronald_Reagan,links_James_Carter]
for index in range(len(links)):
  for i in range(len(links[index])):
    print(str(index+1) + "-" + str(i))
    artical=""
    average_list=[]
    x=1
    mean=0

    options = webdriver.ChromeOptions()
    options.add_argument('--headless')
    options.add_argument('--no-sandbox')
    options.add_argument('--disable-dev-shm-usage')
    # open it, go to a website, and get results
    wd = webdriver.Chrome(options=options)
    wd.get(links[index][i])
    webpage=wd.page_source
    wd.quit()
    soup = bs4(webpage)



    #page = requests.get(links[index][i])
    #time.sleep(3) # Sleep for 3 seconds
    #webpage = page.content
    #page.close()
    #soup = bs4(webpage, "html.parser")
    Politician_herf = []

    if (i%7==0):
      Politicians = soup.findAll("div",{"class": "summary"})
      for i2 in range(len(Politicians)):
        for herfs in Politicians[i2].findAll("a",href=True):
          Politician_herf.append(herfs.get('href'))
      for i3 in range(10): #this website has 20 per page and that takes forever 
        artical=""
        page = requests.get(Politician_herf[i3])
        time.sleep(3) # Sleep for 3 seconds
        webpage = page.content
        page.close()
        soup = bs4(webpage, "html.parser")
        text=soup.findAll("p",{"class": " story-text__paragraph"})
        for i4 in range(len(text)):
          artical= artical + "" + text[i4].text
        mean = mean + sentiment_scores(artical)
      if (len(Politician_herf)==0):
        ratings.append(["sourse code is JavaScript"])
      else:
        average=mean/len(Politician_herf)
        ratings.append([average])
      
    if (i%7==1):
      Politicians = soup.findAll("div",{"class": "ssrcss-1cbga70-Stack e1y4nx260"})
      for i2 in range(len(Politicians)):
        for herfs in Politicians[i2].findAll("a",href=True):
          Politician_herf.append(herfs.get('href'))
      for i3 in range(len(Politician_herf)):  
        artical=""
        page = requests.get(Politician_herf[i3])
        time.sleep(3) # Sleep for 3 seconds
        webpage = page.content
        page.close()
        soup = bs4(webpage, "html.parser")
        if (soup.find("p",{"class": "ssrcss-1q0x1qg-Paragraph eq5iqo00"}) is not None):
          text=soup.findAll("p",{"class": "ssrcss-1q0x1qg-Paragraph eq5iqo00"})
        for i4 in range(len(text)):
          artical= artical + "" + text[i4].text
        mean = mean + sentiment_scores(artical)
      if (len(Politician_herf)==0):
        ratings[-1].append("sourse code is JavaScript")
      else:
        average=mean/len(Politician_herf)
        ratings[-1].append(average)
      
    if (i%7==2):
      Politicians = soup.findAll("div",{"class": "cnn-search__result-thumbnail"})
      for i2 in range(len(Politicians)):
        herf=Politicians[i2].a.get('href')
        Politician_herf.append('https:' + str(herf))
      for i3 in range(len(Politician_herf)):
        artical="" 
        page = requests.get(Politician_herf[i3])
        time.sleep(3) # Sleep for 3 seconds
        webpage = page.content
        page.close()
        soup = bs4(webpage, "html.parser")
        text=soup.findAll("p",{"class": "paragraph inline-placeholder"})
        if (len(text)==0):
          text=soup.find("div",{"class": "l-container"})
          if (str(type(text))!="<class 'NoneType'>"):
            for i4 in text:
              artical = artical + "" + i4.text
        else:
          for i4 in range(len(text)):
            artical = artical + "" + text[i4].text
        mean = mean + sentiment_scores(artical)
      if (len(Politician_herf)==0):
        ratings[-1].append("sourse code is JavaScript")
      else:
        average=mean/len(Politician_herf)
        ratings[-1].append(average)
            
    if (i%7==3):
      Politicians = soup.findAll("h2",{"class": "title"})
      for i2 in range(len(Politicians)):
        for herfs in Politicians[i2].findAll("a",href=True):
          Politician_herf.append(herfs.get('href'))
      for i3 in range(len(Politician_herf)):  
        artical=""
        page = requests.get(Politician_herf[i3])
        time.sleep(3) # Sleep for 3 seconds
        webpage = page.content
        page.close()
        soup = bs4(webpage, "html.parser")
        text=soup.findAll("p")
        for i4 in range(len(text)):
          artical= artical + "" + text[i4].text
        mean = mean + sentiment_scores(artical)
      if (len(Politician_herf)==0):
        ratings[-1].append("sourse code is JavaScript")
      else:
        average=mean/len(Politician_herf)
        ratings[-1].append(average)
      
    if (i%7==4):
      Politicians = Politicians = soup.body.findAll("a",{"class":"card__headline card__headline--long"})
      for i2 in Politicians:
        Politician_herf.append(i2['href'])
      for i3 in Politician_herf: 
        artical=""
        options = webdriver.ChromeOptions()
        options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        # open it, go to a website, and get results
        wd = webdriver.Chrome(options=options)
        wd.get(i3)
        webpage=wd.page_source
        wd.quit()
        soup = bs4(webpage)
        text=soup.findAll("div",{"class": "primary-cli cli cli-text "})
        for i4 in range(len(text)):
          artical= artical + "" + text[i4].text
        mean = mean + sentiment_scores(artical)
      if (len(Politician_herf)==0):
        ratings[-1].append("sourse code is JavaScript")
      else:
        average=mean/len(Politician_herf)
        ratings[-1].append(average)
            
    if (i%7==5):
      Politicians = soup.findAll("div",{"class": "pr-sm flex flex-column justify-between w-100"})
      for i2 in range(len(Politicians)):
        for herfs in Politicians[i2].findAll("a",href=True):
          Politician_herf.append(herfs.get('href'))
      for i3 in range(len(Politician_herf)):  
        artical=""
        page = requests.get(Politician_herf[i3])
        time.sleep(3) # Sleep for 3 seconds
        webpage = page.content
        page.close()
        soup = bs4(webpage, "html.parser")
        text=soup.findAll("p",{"class": "font-copy font--article-body gray-darkest ma-0 pb-md"})
        for i4 in range(len(text)):
          artical= artical + "" + text[i4].text
        mean = mean + sentiment_scores(artical)
      if (len(Politician_herf)==0):
        ratings[-1].append("sourse code is JavaScript")
      else:
        average=mean/len(Politician_herf)
        ratings[-1].append(average)
    print(ratings[-1][-1])

pres = ["Donald Trump","Joe Biden","Barack Obama","George W. Bush","William J. Clinton","Ronald Reagan","James Carter"]
df_ratings=pandas.DataFrame(ratings)
df_ratings.columns = df_ratings.iloc[0]
df_ratings = df_ratings[1:]
df_ratings.index = pres
df_ratings = df_ratings.drop('NPR', 1)
l=[4,5,6]
for i in l:
  df_ratings["The Washington Post"].iloc[i]=0
df_ratings

import seaborn as sns

df = df_ratings.astype(float)
sns.heatmap(df, cmap ='YlGnBu', annot = True)
plt.title("presidents positivity scores")
plt.show